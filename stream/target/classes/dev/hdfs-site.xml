<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->

<!-- Put site-specific property overrides in this file. -->


				<configuration>
			
					 <!--这里的主机名一定要和你的Linux虚拟机的主机名保持一致-->
					 <property>
							<name>dfs.namenode.secondary.http-address</name>
							<value>node01:50090</value>
					</property>
					
					<!--定义namenode的访问地址-->
					<property>
						<name>dfs.namenode.http-address</name>
						<value>node01:50070</value>
					</property>
					<!--主要用于存放我们的元数据信息的地址, 注意这种写法, 实际工作当中一定要先确认你的Linux的磁盘挂载在哪个路径下面-->
					<!--这些都是我们的fsimage的存放位置-->
					<property>
						<name>dfs.namenode.name.dir</name>
						<value>file:////export/servers/hadoop-2.6.0-cdh5.14.0/hadoopDatas/namenodeDatas</value>
					</property>
					<!--  定义dataNode数据存储的节点位置，实际工作中，一般先确定磁盘的挂载目录，然后多个目录用，进行分割,一定不要把数据干到系统盘中  -->
					<property>
						<name>dfs.datanode.data.dir</name>
						<value>file:////export/servers/hadoop-2.6.0-cdh5.14.0/hadoopDatas/datanodeDatas,file:////export/servers/hadoop-2.6.0-cdh5.14.0/hadoopDatas/datanodeDatas2</value>
					</property>
					<!--定义我们的元数据的操作日志的存放位置, 也就是edits的存放位置-->
					<property>
						<name>dfs.namenode.edits.dir</name>
						<value>file:////export/servers/hadoop-2.6.0-cdh5.14.0/hadoopDatas/nn/edits</value>
					</property>
					
					<!--元数据检查点的保存位置, 什么是检查点, 明天详细讲解-->
					<property>
						<name>dfs.namenode.checkpoint.dir</name>
						<value>file:////export/servers/hadoop-2.6.0-cdh5.14.0/hadoopDatas/snn/name</value>
					</property>
					<!--edits文件的检查点保存位置-->
					<property>
						<name>dfs.namenode.checkpoint.edits.dir</name>
						<value>file:////export/servers/hadoop-2.6.0-cdh5.14.0/hadoopDatas/dfs/snn/edits</value>
					</property>
					<!--文件的副本数, 一个文件保存多少份-->
					<property>
						<name>dfs.replication</name>
						<value>3</value>
					</property>
					<!--hdfs的权限控制, false: 关闭权限, true: 开启权限-->
					<property>
						<name>dfs.permissions</name>
						<value>false</value>
					</property>
					<!--文件存储的block块大小-->
					<property>
						<name>dfs.blocksize</name>
						<value>134217728</value>
					</property>
					
					<property>
			           <name>dfs.client.read.shortcircuit</name>
			           <value>true</value>
		            </property>
					<property>
						<name>dfs.domain.socket.path</name>
						<value>/var/run/hdfs-sockets/dn</value>
					</property>
					<property>
						<name>dfs.client.file-block-storage-locations.timeout.millis</name>
						<value>10000</value>
					</property>
					<property>
						<name>dfs.datanode.hdfs-blocks-metadata.enabled</name>
						<value>true</value>
					</property>
					
					<property>
					  <name>dfs.webhdfs.enabled</name>
					  <value>true</value>
					</property>
				</configuration>
